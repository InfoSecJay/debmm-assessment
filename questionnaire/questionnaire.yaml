# DEBMM Assessment Questionnaire
# ~55 questions mapped to 21 rubric criteria across 7 categories
#
# Question types:
#   checklist - yes/no binary (scored: yes=yes_value, no=1)
#   scale     - 1-5 maturity self-rating (scored directly)
#   text      - free-form written answer (flagged for manual/LLM review)
#
# Each question has two phrasings:
#   question       - self-assessment ("your team")
#   question_audit - audit-style ("the assessed team")

questions:

  # ============================================================
  # TIER 0: FOUNDATION
  # ============================================================

  # --- Structured Rule Development ---
  - id: T0-Q1
    tier: 0
    criterion: structured_rule_development
    type: checklist
    question: "Does your team follow a documented methodology for developing detection rules?"
    question_audit: "Does the assessed team follow a documented methodology for developing detection rules?"
    scoring:
      yes_value: 3

  - id: T0-Q2
    tier: 0
    criterion: structured_rule_development
    type: checklist
    question: "Do new detection rules go through peer review before deployment?"
    question_audit: "Do new detection rules go through peer review before deployment?"
    scoring:
      yes_value: 4

  - id: T0-Q3
    tier: 0
    criterion: structured_rule_development
    type: scale
    question: "Rate the maturity of your rule development process."
    question_audit: "Rate the maturity of the assessed team's rule development process."
    options:
      1: "No structured approach; rules created ad hoc"
      2: "Some rules follow a loose process, inconsistently applied"
      3: "Defined methodology documented and followed for most rules"
      4: "Standardized across team with enforced workflows and quality gates"
      5: "Continuous improvement with CI/CD integration and automated validation"

  # --- Rule Creation and Maintenance ---
  - id: T0-Q4
    tier: 0
    criterion: rule_creation_maintenance
    type: checklist
    question: "Do your detection rules have assigned owners responsible for their maintenance?"
    question_audit: "Do the assessed team's detection rules have assigned owners?"
    scoring:
      yes_value: 3

  - id: T0-Q5
    tier: 0
    criterion: rule_creation_maintenance
    type: scale
    question: "What percentage of your detection rules are reviewed on a regular schedule?"
    question_audit: "What percentage of the assessed team's rules are reviewed on a regular schedule?"
    options:
      1: "Less than 50% reviewed annually"
      2: "50-70% reviewed annually"
      3: "70-80% reviewed on schedule"
      4: "80-90% reviewed on schedule with peer review on all changes"
      5: "90-100% reviewed with automated health monitoring"

  # --- Roadmap Documentation ---
  - id: T0-Q6
    tier: 0
    criterion: roadmap_documentation
    type: checklist
    question: "Does your team maintain a documented detection engineering roadmap?"
    question_audit: "Does the assessed team maintain a documented detection engineering roadmap?"
    scoring:
      yes_value: 3

  - id: T0-Q7
    tier: 0
    criterion: roadmap_documentation
    type: scale
    question: "Rate the maturity of your detection engineering roadmap."
    question_audit: "Rate the maturity of the assessed team's detection roadmap."
    options:
      1: "No roadmap exists; work is entirely reactive"
      2: "Informal roadmap or backlog exists but not regularly maintained"
      3: "Formal roadmap reviewed quarterly and shared with stakeholders"
      4: "Integrated with security strategy; progress tracked with metrics"
      5: "Dynamic, continuously updated based on intel, gaps, and risk"

  # --- Threat Modeling ---
  - id: T0-Q8
    tier: 0
    criterion: threat_modeling
    type: checklist
    question: "Does your team perform threat modeling exercises to inform detection priorities?"
    question_audit: "Does the assessed team perform threat modeling exercises?"
    scoring:
      yes_value: 3

  - id: T0-Q9
    tier: 0
    criterion: threat_modeling
    type: scale
    question: "How frequently does your team perform threat modeling?"
    question_audit: "How frequently does the assessed team perform threat modeling?"
    options:
      1: "Never or not at all"
      2: "Less than once per year"
      3: "Quarterly with documented results"
      4: "Monthly; outputs directly inform detection priorities"
      5: "Continuous with real-time threat intelligence integration"

  - id: T0-Q10
    tier: 0
    criterion: threat_modeling
    type: text
    question: "Describe your threat modeling process, including frameworks used, frequency, and how outputs inform detection development."
    question_audit: "Describe the assessed team's threat modeling process, including frameworks used, frequency, and how outputs inform detection development."

  # ============================================================
  # TIER 1: BASIC
  # ============================================================

  # --- Baseline Rule Creation ---
  - id: T1-Q1
    tier: 1
    criterion: baseline_rule_creation
    type: scale
    question: "How many custom baseline detection rules does your team maintain?"
    question_audit: "How many custom baseline detection rules does the assessed team maintain?"
    options:
      1: "Fewer than 10 custom rules; mostly vendor defaults"
      2: "10-30 rules covering critical threats"
      3: "30-60 rules with a mix of signature and behavioral detections"
      4: "60-100 rules including behavioral and TTP-focused detections"
      5: "100+ rules with continuous refinement and environment-specific tuning"

  - id: T1-Q2
    tier: 1
    criterion: baseline_rule_creation
    type: scale
    question: "Estimate your MITRE ATT&CK technique coverage for your priority threat areas."
    question_audit: "Estimate the assessed team's MITRE ATT&CK technique coverage for priority threat areas."
    options:
      1: "Minimal or unknown coverage"
      2: "Less than 30% of priority techniques covered"
      3: "40-60% of priority techniques covered"
      4: "60-80% coverage with behavioral detections"
      5: "Over 80% coverage with continuous gap analysis"

  # --- Ruleset Management ---
  - id: T1-Q3
    tier: 1
    criterion: ruleset_management
    type: checklist
    question: "Are your detection rules stored in a version control system (e.g., Git)?"
    question_audit: "Are the assessed team's detection rules stored in version control?"
    scoring:
      yes_value: 3

  - id: T1-Q4
    tier: 1
    criterion: ruleset_management
    type: checklist
    question: "Does your team practice detection-as-code with CI/CD pipelines for rule deployment?"
    question_audit: "Does the assessed team practice detection-as-code with CI/CD pipelines?"
    scoring:
      yes_value: 4

  - id: T1-Q5
    tier: 1
    criterion: ruleset_management
    type: scale
    question: "Rate the maturity of your ruleset management and maintenance practices."
    question_audit: "Rate the maturity of the assessed team's ruleset management."
    options:
      1: "No formal management; rules live in the SIEM without version control"
      2: "Some rules in version control with basic documentation"
      3: "Most rules versioned with documentation standards; DaC being adopted"
      4: "DaC is standard; CI/CD handles deployment; all documented"
      5: "Fully automated lifecycle with CI/CD, testing, and weekly validation"

  # --- Telemetry Quality ---
  - id: T1-Q6
    tier: 1
    criterion: telemetry_quality
    type: scale
    question: "Rate the quality and coverage of your telemetry data sources."
    question_audit: "Rate the quality and coverage of the assessed team's telemetry."
    options:
      1: "No active management; data sources unassessed"
      2: "Some awareness of gaps; basic health checks on critical sources"
      3: "Actively monitored; coverage mapped to detection needs; CTI integration starting"
      4: "Comprehensive with automated monitoring and CTI enrichment"
      5: "Advanced workflows with real-time enrichment and automated gap remediation"

  - id: T1-Q7
    tier: 1
    criterion: telemetry_quality
    type: text
    question: "Describe your telemetry management practices. How do you ensure data quality and identify coverage gaps?"
    question_audit: "Describe the assessed team's telemetry management. How is data quality ensured and coverage gaps identified?"

  # --- Threat Landscape Review ---
  - id: T1-Q8
    tier: 1
    criterion: threat_landscape_review
    type: scale
    question: "How frequently does your team review the threat landscape to update detection priorities?"
    question_audit: "How frequently does the assessed team review the threat landscape?"
    options:
      1: "No regular reviews"
      2: "Annually or bi-annually"
      3: "Quarterly with documented findings updating the roadmap"
      4: "Monthly integrated with threat intelligence"
      5: "Real-time monitoring with automated intel feeds"

  # --- Product Owner Engagement ---
  - id: T1-Q9
    tier: 1
    criterion: product_owner_engagement
    type: scale
    question: "Rate the level of engagement between your detection engineering team and security product/platform owners."
    question_audit: "Rate the engagement between the assessed team and product/platform owners."
    options:
      1: "No engagement with product owners"
      2: "Occasional ad hoc engagement, reactive to issues"
      3: "Regular engagement to communicate needs and provide feedback"
      4: "Proactive partnership; detection needs on product roadmaps"
      5: "Continuous engagement with joint planning and shared metrics"

  # --- Release Testing ---
  - id: T1-Q10
    tier: 1
    criterion: release_testing
    type: checklist
    question: "Does your team test detection rules before deploying them to production?"
    question_audit: "Does the assessed team test rules before production deployment?"
    scoring:
      yes_value: 3

  - id: T1-Q11
    tier: 1
    criterion: release_testing
    type: scale
    question: "Rate the maturity of your release testing and validation process."
    question_audit: "Rate the maturity of the assessed team's release testing."
    options:
      1: "No formal testing before deployment"
      2: "Basic manual testing on some rules"
      3: "Standardized testing with defined test cases and staging environment"
      4: "Comprehensive testing with unit, integration, and emulation validation"
      5: "Continuous automated testing in full CI/CD pipeline"

  # ============================================================
  # TIER 2: INTERMEDIATE
  # ============================================================

  # --- False Positive Reduction ---
  - id: T2-Q1
    tier: 2
    criterion: false_positive_reduction
    type: checklist
    question: "Does your team track false positive rates per detection rule?"
    question_audit: "Does the assessed team track false positive rates per rule?"
    scoring:
      yes_value: 3

  - id: T2-Q2
    tier: 2
    criterion: false_positive_reduction
    type: scale
    question: "Rate the maturity of your false positive tuning and reduction efforts."
    question_audit: "Rate the maturity of the assessed team's FP reduction efforts."
    options:
      1: "Minimal or no tuning; high FP rates accepted"
      2: "Some reactive tuning when analysts complain"
      3: "Regular tuning cycles with tracked FP rates per rule"
      4: "Comprehensive FP management with automated tuning and risk scoring"
      5: "Automated dynamic tuning with ML; near-zero unnecessary noise"

  - id: T2-Q3
    tier: 2
    criterion: false_positive_reduction
    type: text
    question: "Describe your false positive management process. What metrics do you track and what reduction have you achieved?"
    question_audit: "Describe the assessed team's FP management process, metrics tracked, and reduction achieved."

  # --- Gap Analysis ---
  - id: T2-Q4
    tier: 2
    criterion: gap_analysis
    type: checklist
    question: "Does your team perform regular gap analysis against frameworks like MITRE ATT&CK?"
    question_audit: "Does the assessed team perform regular gap analysis against ATT&CK or similar?"
    scoring:
      yes_value: 3

  - id: T2-Q5
    tier: 2
    criterion: gap_analysis
    type: scale
    question: "Rate the maturity of your detection coverage gap analysis."
    question_audit: "Rate the maturity of the assessed team's gap analysis."
    options:
      1: "No gap analysis; coverage gaps unknown"
      2: "Some gaps identified informally after incidents"
      3: "Regular analysis against frameworks; gaps documented and prioritized"
      4: "Comprehensive analysis integrated with threat modeling and risk"
      5: "Automated analysis with real-time coverage dashboards"

  - id: T2-Q6
    tier: 2
    criterion: gap_analysis
    type: text
    question: "Describe your gap analysis process. How are gaps identified, documented, prioritized, and communicated?"
    question_audit: "Describe the assessed team's gap analysis process and how gaps are tracked and communicated."

  # --- Internal Testing ---
  - id: T2-Q7
    tier: 2
    criterion: internal_testing
    type: checklist
    question: "Does your team conduct attack emulation or simulation to validate detection rules?"
    question_audit: "Does the assessed team conduct attack emulation/simulation for validation?"
    scoring:
      yes_value: 3

  - id: T2-Q8
    tier: 2
    criterion: internal_testing
    type: scale
    question: "Rate the maturity of your internal detection testing and validation program."
    question_audit: "Rate the maturity of the assessed team's internal testing program."
    options:
      1: "No internal testing of detection effectiveness"
      2: "Occasional manual testing of high-priority rules"
      3: "Regular testing with attack emulation; results documented"
      4: "Comprehensive with automated emulation and purple team exercises"
      5: "Continuous automated testing with full emulation and regression"

  # ============================================================
  # TIER 3: ADVANCED
  # ============================================================

  # --- False Negative Triage ---
  - id: T3-Q1
    tier: 3
    criterion: false_negative_triage
    type: checklist
    question: "Does your team have a process for identifying and triaging false negatives (missed detections)?"
    question_audit: "Does the assessed team have a process for identifying and triaging false negatives?"
    scoring:
      yes_value: 3

  - id: T3-Q2
    tier: 3
    criterion: false_negative_triage
    type: scale
    question: "Rate your team's ability to detect and remediate false negatives."
    question_audit: "Rate the assessed team's ability to detect and remediate false negatives."
    options:
      1: "No FN identification; missed detections only found during incidents"
      2: "Some FNs identified through post-incident reviews"
      3: "Systematic FN identification through regular testing; root causes analyzed"
      4: "Comprehensive with automated validation and rapid remediation"
      5: "Continuous automated FN detection with real-time validation"

  - id: T3-Q3
    tier: 3
    criterion: false_negative_triage
    type: text
    question: "Describe how your team identifies, tracks, and remediates false negatives. What is your detection trigger rate on tested samples?"
    question_audit: "Describe the assessed team's FN identification and remediation process and detection trigger rates."

  # --- External Validation ---
  - id: T3-Q4
    tier: 3
    criterion: external_validation
    type: checklist
    question: "Does your organization conduct external validation of detection capabilities (red team, pentest, BAS)?"
    question_audit: "Does the assessed organization conduct external validation of detections?"
    scoring:
      yes_value: 3

  - id: T3-Q5
    tier: 3
    criterion: external_validation
    type: scale
    question: "Rate the maturity of your external validation program."
    question_audit: "Rate the maturity of the assessed team's external validation."
    options:
      1: "No external validation of detections"
      2: "Annual pentest with some detection assessment"
      3: "Regular red team or third-party assessments focused on detection"
      4: "Multiple annual exercises (red/purple team, breach simulation)"
      5: "Continuous BAS tools and regular adversary emulation"

  # --- Advanced TTP Coverage ---
  - id: T3-Q6
    tier: 3
    criterion: advanced_ttp_coverage
    type: scale
    question: "Rate your detection coverage of advanced TTPs (living-off-the-land, fileless, evasion techniques)."
    question_audit: "Rate the assessed team's coverage of advanced TTPs."
    options:
      1: "No advanced TTP coverage; signatures and IOCs only"
      2: "Limited coverage of 1-3 advanced techniques"
      3: "Growing coverage informed by intel; behavioral detections in place"
      4: "Comprehensive coverage of evasion, novel attack chains, emerging threats"
      5: "Continuous proactive coverage using AI/ML for anomaly detection"

  - id: T3-Q7
    tier: 3
    criterion: advanced_ttp_coverage
    type: text
    question: "Describe your advanced TTP detection capabilities. What specific advanced techniques do you detect and how?"
    question_audit: "Describe the assessed team's advanced TTP detection capabilities and specific techniques covered."

  # ============================================================
  # TIER 4: EXPERT
  # ============================================================

  # --- Threat Hunting ---
  - id: T4-Q1
    tier: 4
    criterion: threat_hunting
    type: checklist
    question: "Does your team conduct proactive threat hunting activities?"
    question_audit: "Does the assessed team conduct proactive threat hunting?"
    scoring:
      yes_value: 3

  - id: T4-Q2
    tier: 4
    criterion: threat_hunting
    type: scale
    question: "Rate the maturity of your threat hunting program."
    question_audit: "Rate the maturity of the assessed team's threat hunting."
    options:
      1: "No proactive hunting; all detection is passive"
      2: "Occasional ad hoc hunting; findings not systematically converted"
      3: "Regular structured hunting driven by intelligence; findings feed detections"
      4: "Comprehensive daily program with advanced analytics and integration"
      5: "Automated real-time hunting augmented by AI/ML"

  - id: T4-Q3
    tier: 4
    criterion: threat_hunting
    type: text
    question: "Describe your threat hunting program. How often do you hunt, what drives your hypotheses, and how are findings integrated into detections?"
    question_audit: "Describe the assessed team's hunting program, hypothesis process, and finding integration."

  # --- Automation & Continuous Improvement ---
  - id: T4-Q4
    tier: 4
    criterion: automation_continuous_improvement
    type: scale
    question: "What percentage of your detection engineering tasks are automated?"
    question_audit: "What percentage of the assessed team's DE tasks are automated?"
    options:
      1: "None; all processes are manual"
      2: "Less than 30% (basic deployment scripts)"
      3: "40-60% (lifecycle automation; improvement metrics tracked)"
      4: "70-80% (advanced automation; AI/LLM tools in use)"
      5: "Over 90% (full AI/LLM lifecycle integration)"

  - id: T4-Q5
    tier: 4
    criterion: automation_continuous_improvement
    type: checklist
    question: "Does your team use AI or LLM tools to assist with detection rule development, tuning, or analysis?"
    question_audit: "Does the assessed team use AI/LLM tools for detection work?"
    scoring:
      yes_value: 4

  - id: T4-Q6
    tier: 4
    criterion: automation_continuous_improvement
    type: text
    question: "Describe your automation and AI/LLM integration in the detection engineering lifecycle. What tools do you use and what results have you achieved?"
    question_audit: "Describe the assessed team's automation and AI/LLM integration, tools used, and results."

  # ============================================================
  # ENRICHMENT: PEOPLE & ORGANIZATION
  # ============================================================

  # --- Team Structure ---
  - id: EP-Q1
    tier: enrichment_people
    criterion: team_structure
    type: scale
    question: "Rate the maturity of your detection engineering team structure."
    question_audit: "Rate the maturity of the assessed detection engineering team structure."
    options:
      1: "No dedicated roles; detection is a side task for other staff"
      2: "Part-time detection responsibilities; no formal team"
      3: "Dedicated role(s) or team with clear responsibilities"
      4: "Established team with domain experts (host, network, cloud, app)"
      5: "Mature team with specialization, mentorship, and strategic influence"

  - id: EP-Q2
    tier: enrichment_people
    criterion: team_structure
    type: checklist
    question: "Does your organization have at least one full-time dedicated detection engineer?"
    question_audit: "Does the assessed organization have at least one dedicated detection engineer?"
    scoring:
      yes_value: 3

  # --- Skills Development ---
  - id: EP-Q3
    tier: enrichment_people
    criterion: skills_development
    type: scale
    question: "Rate the maturity of your detection engineering skills development program."
    question_audit: "Rate the maturity of the assessed team's skills development."
    options:
      1: "No formal training; self-directed learning only"
      2: "Ad hoc training; no structured program"
      3: "Defined training program with regular knowledge sharing"
      4: "Comprehensive training with cross-training and certifications"
      5: "Continuous learning culture with community contribution and research"

  - id: EP-Q4
    tier: enrichment_people
    criterion: skills_development
    type: checklist
    question: "Does your team have a defined training plan or skills development roadmap?"
    question_audit: "Does the assessed team have a defined training plan or skills roadmap?"
    scoring:
      yes_value: 3

  # --- Leadership Commitment ---
  - id: EP-Q5
    tier: enrichment_people
    criterion: leadership_commitment
    type: scale
    question: "Rate the level of executive sponsorship and leadership commitment to detection engineering."
    question_audit: "Rate the executive sponsorship of the assessed detection engineering function."
    options:
      1: "No executive awareness or sponsorship"
      2: "Some awareness but no formal sponsorship or budget"
      3: "Executive sponsor; dedicated budget; function formally recognized"
      4: "Strong support; metrics reported to leadership; influences investments"
      5: "Strategic priority; board-level visibility; cross-org influence"

  - id: EP-Q6
    tier: enrichment_people
    criterion: leadership_commitment
    type: checklist
    question: "Does your detection engineering function have a dedicated budget?"
    question_audit: "Does the assessed detection engineering function have a dedicated budget?"
    scoring:
      yes_value: 3

  - id: EP-Q7
    tier: enrichment_people
    criterion: leadership_commitment
    type: text
    question: "Describe executive engagement with detection engineering. How is the function's value communicated to and supported by leadership?"
    question_audit: "Describe executive engagement with the assessed detection engineering function."

  # ============================================================
  # ENRICHMENT: PROCESS & GOVERNANCE
  # ============================================================

  # --- Detection Lifecycle ---
  - id: EG-Q1
    tier: enrichment_process
    criterion: detection_lifecycle
    type: scale
    question: "Rate the maturity of your detection lifecycle workflow (from request through retirement)."
    question_audit: "Rate the maturity of the assessed team's detection lifecycle workflow."
    options:
      1: "No defined lifecycle; no structured workflow"
      2: "Basic lifecycle covering creation and deployment only"
      3: "Full lifecycle: request, development, review, testing, deploy, monitor, retire"
      4: "Enforced through tooling; SLAs defined; cycle time tracked"
      5: "Optimized with automated transitions and predictive analytics"

  - id: EG-Q2
    tier: enrichment_process
    criterion: detection_lifecycle
    type: checklist
    question: "Does your team have a defined process for retiring or deprecating outdated detection rules?"
    question_audit: "Does the assessed team have a rule retirement/deprecation process?"
    scoring:
      yes_value: 3

  # --- Metrics & KPIs ---
  - id: EG-Q3
    tier: enrichment_process
    criterion: metrics_tracking
    type: checklist
    question: "Does your team track detection engineering KPIs (e.g., coverage, quality, velocity)?"
    question_audit: "Does the assessed team track detection engineering KPIs?"
    scoring:
      yes_value: 3

  - id: EG-Q4
    tier: enrichment_process
    criterion: metrics_tracking
    type: scale
    question: "Rate the maturity of your detection engineering metrics program."
    question_audit: "Rate the maturity of the assessed team's metrics program."
    options:
      1: "No metrics tracked; success is anecdotal"
      2: "Basic metrics tracked informally (rule count, alert volume)"
      3: "Defined KPIs for coverage, quality, velocity; quarterly reporting"
      4: "Comprehensive program with automated collection; metrics drive decisions"
      5: "Advanced analytics with predictive metrics and benchmarking"

  - id: EG-Q5
    tier: enrichment_process
    criterion: metrics_tracking
    type: text
    question: "List the key metrics and KPIs your team tracks for detection engineering. How are they collected, reported, and used?"
    question_audit: "List the KPIs tracked by the assessed team and describe collection and reporting."

  # --- Cross-Team Collaboration ---
  - id: EG-Q6
    tier: enrichment_process
    criterion: cross_team_collaboration
    type: scale
    question: "Rate the maturity of collaboration between detection engineering and other teams (IR, threat intel, engineering)."
    question_audit: "Rate cross-team collaboration involving the assessed detection engineering function."
    options:
      1: "Operates in isolation; no structured collaboration"
      2: "Ad hoc collaboration, reactive to incidents"
      3: "Regular collaboration via defined channels and scheduled touchpoints"
      4: "Deep integration with joint planning and shared objectives"
      5: "Seamless cross-functional collaboration with automated information sharing"

  - id: EG-Q7
    tier: enrichment_process
    criterion: cross_team_collaboration
    type: checklist
    question: "Does your detection engineering team have regularly scheduled touchpoints with incident response and threat intelligence teams?"
    question_audit: "Does the assessed team have regular touchpoints with IR and threat intel?"
    scoring:
      yes_value: 3
