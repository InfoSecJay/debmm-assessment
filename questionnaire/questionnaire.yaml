# DEBMM Assessment Questionnaire v2
# 41 questions mapped to 21 rubric criteria across 7 categories
#
# All questions are dropdowns (scale 1-5 or checklist yes/no).
# No text/free-form questions. Quantitative thresholds from the rubric
# are embedded directly in scale option labels to eliminate ambiguity.
#
# Question types:
#   checklist - yes/no (scored: yes=yes_value, no=1)
#   scale     - 1-5 maturity self-rating (scored directly)
#
# Each question has two phrasings:
#   question       - self-assessment ("your team")
#   question_audit - audit-style ("the assessed team")

questions:

  # ============================================================
  # TIER 0: FOUNDATION
  # ============================================================

  # --- Structured Rule Development ---
  - id: T0-Q1
    tier: 0
    criterion: structured_rule_development
    type: scale
    question: "Rate the maturity of your team's rule development process."
    question_audit: "Rate the maturity of the assessed team's rule development process."
    options:
      1: "No structured approach; rules created ad hoc or reactively (0% follow a formal process)"
      2: "Some rules follow a loose process but it is inconsistently applied (<30% follow a documented process)"
      3: "Defined methodology is documented and followed for most new rules (50-70% follow process; schema alignment >60%)"
      4: "Standardized across team with enforced workflows, templates, and quality gates (80-90% schema alignment; all new rules go through formal review)"
      5: "Continuous improvement via feedback loops; fully integrated with CI/CD and automated linting/validation (90-100% schema alignment)"

  - id: T0-Q2
    tier: 0
    criterion: structured_rule_development
    type: checklist
    question: "Do all new detection rules go through peer review before deployment to production?"
    question_audit: "Do all new detection rules go through peer review before production deployment?"
    scoring:
      yes_value: 4

  # --- Rule Creation and Maintenance ---
  - id: T0-Q3
    tier: 0
    criterion: rule_creation_maintenance
    type: scale
    question: "What percentage of your detection rules are reviewed on a regular schedule?"
    question_audit: "What percentage of the assessed team's rules are reviewed on a regular schedule?"
    options:
      1: "Less than 50% reviewed annually; no rule owners assigned"
      2: "50-70% reviewed annually; rules may have informal owners but no formal peer review process"
      3: "70-80% reviewed on schedule; rules have assigned owners and defined review cycles; peer review on most changes"
      4: "80-90% reviewed on schedule; comprehensive lifecycle management (create, test, deploy, monitor, retire); 100% peer review on changes"
      5: "90-100% reviewed on schedule; automated rule health monitoring flags stale or broken rules for review"

  # --- Roadmap Documentation ---
  - id: T0-Q4
    tier: 0
    criterion: roadmap_documentation
    type: scale
    question: "Rate the maturity of your detection engineering roadmap."
    question_audit: "Rate the maturity of the assessed team's detection engineering roadmap."
    options:
      1: "No roadmap exists; work is entirely reactive or driven by individual initiative"
      2: "Informal roadmap or backlog exists (e.g., a Jira board or wiki page) but is not regularly maintained or shared (<30% of work tied to a plan)"
      3: "Formal roadmap documented, reviewed at least quarterly, shared with stakeholders; priorities are clear (50-70% of planned work tracked)"
      4: "Integrated with organizational security strategy; progress tracked with metrics and reported to leadership (70-90% tracked; quarterly leadership reviews)"
      5: "Dynamic, continuously updated roadmap driven by threat intel, gap analysis, and risk priorities (90-100% tracked; auto-updated from analysis feeds)"

  # --- Threat Modeling ---
  - id: T0-Q5
    tier: 0
    criterion: threat_modeling
    type: scale
    question: "How frequently does your team perform threat modeling to inform detection priorities?"
    question_audit: "How frequently does the assessed team perform threat modeling?"
    options:
      1: "Never performed or not at all"
      2: "Less than once per year; typically triggered by a major incident or audit finding"
      3: "1-2 times per year with documented results that inform the detection roadmap"
      4: "Quarterly or more; outputs directly prioritize new detection development (>70% of new detections tied to threat model outputs)"
      5: "Continuous proactive modeling incorporating real-time threat intel, attack surface changes, and emerging TTPs"

  - id: T0-Q6
    tier: 0
    criterion: threat_modeling
    type: checklist
    question: "Does your threat modeling use a recognized framework (e.g., STRIDE, MITRE ATT&CK, PASTA, attack trees)?"
    question_audit: "Does the assessed team's threat modeling use a recognized framework?"
    scoring:
      yes_value: 3

  - id: T0-Q7
    tier: 0
    criterion: threat_modeling
    type: checklist
    question: "Do threat modeling outputs directly generate items on your detection engineering backlog or roadmap?"
    question_audit: "Do threat modeling outputs directly generate items on the assessed team's backlog?"
    scoring:
      yes_value: 4

  # ============================================================
  # TIER 1: BASIC
  # ============================================================

  # --- Baseline Rule Creation ---
  - id: T1-Q1
    tier: 1
    criterion: baseline_rule_creation
    type: scale
    question: "How many custom detection rules does your team maintain (excluding vendor defaults)?"
    question_audit: "How many custom detection rules does the assessed team maintain?"
    options:
      1: "Fewer than 10 custom rules; mostly relying on vendor/out-of-the-box rules"
      2: "10-30 rules covering the most critical threats (e.g., ransomware, credential theft, C2)"
      3: "30-60 rules with a mix of signature-based and behavioral detections across major categories"
      4: "60-100 rules including behavioral and TTP-focused detections tuned per environment"
      5: "100+ rules with continuous refinement, environment-specific tuning, and automated coverage analysis"

  - id: T1-Q2
    tier: 1
    criterion: baseline_rule_creation
    type: scale
    question: "Estimate your MITRE ATT&CK technique coverage for your organization's priority threat areas."
    question_audit: "Estimate the assessed team's MITRE ATT&CK technique coverage for priority areas."
    options:
      1: "Unknown or not measured; no ATT&CK mapping performed"
      2: "Less than 30% of priority techniques covered; coverage is ad hoc"
      3: "30-50% of priority techniques covered with documented gaps identified"
      4: "50-70% coverage including behavioral detections; gaps tracked against threat model"
      5: "Over 70% coverage with continuous gap analysis and automated coverage tracking"

  # --- Ruleset Management ---
  - id: T1-Q3
    tier: 1
    criterion: ruleset_management
    type: scale
    question: "Rate the maturity of your ruleset management practices."
    question_audit: "Rate the maturity of the assessed team's ruleset management."
    options:
      1: "No formal management; rules live only in the SIEM console with no version control or documentation (<20% in version control)"
      2: "Some rules stored in version control (e.g., Git) with basic documentation (20-50% in VCS)"
      3: "Most rules in version control with documentation standards; detection-as-code approach being adopted (50-80% in VCS with docs)"
      4: "Detection-as-code is standard practice; CI/CD pipelines handle rule testing and deployment; all rules documented and versioned (80-90% in DaC pipeline)"
      5: "Fully automated rule lifecycle: CI/CD, automated testing, continuous validation, and weekly maintenance cycles (100% DaC)"

  # --- Telemetry Quality ---
  - id: T1-Q4
    tier: 1
    criterion: telemetry_quality
    type: scale
    question: "Rate the quality and coverage of your telemetry data sources for detection."
    question_audit: "Rate the quality and coverage of the assessed team's telemetry."
    options:
      1: "No active telemetry management; using whatever data sources happen to be available (<30% of rule types have adequate telemetry)"
      2: "Some awareness of gaps; basic health checks on critical sources like EDR and firewall logs (30-50% adequate coverage)"
      3: "Actively monitored; data source coverage mapped to detection needs; CTI enrichment beginning (50-70% coverage)"
      4: "Comprehensive with automated health monitoring, CTI enrichment, and proactive gap identification (70-90% coverage)"
      5: "Advanced workflows with real-time enrichment, automated remediation of telemetry gaps, full CTI integration (90-100% coverage)"

  - id: T1-Q5
    tier: 1
    criterion: telemetry_quality
    type: checklist
    question: "Do you maintain a documented inventory of telemetry data sources mapped to detection use cases?"
    question_audit: "Does the assessed team maintain a documented data source inventory mapped to use cases?"
    scoring:
      yes_value: 3

  - id: T1-Q6
    tier: 1
    criterion: telemetry_quality
    type: checklist
    question: "Do you have automated alerting for when critical data sources stop ingesting or degrade in quality?"
    question_audit: "Does the assessed team have automated alerting for data source degradation?"
    scoring:
      yes_value: 4

  # --- Threat Landscape Review ---
  - id: T1-Q7
    tier: 1
    criterion: threat_landscape_review
    type: scale
    question: "How frequently does your team review the threat landscape to update detection priorities?"
    question_audit: "How frequently does the assessed team review the threat landscape?"
    options:
      1: "No regular reviews; detection priorities are not informed by current threats"
      2: "Annually or bi-annually; some rule updates after major threat advisories (1-2 reviews/year)"
      3: "Quarterly with documented findings that update the detection roadmap (50-70% of rules reviewed against current threats)"
      4: "Monthly reviews integrated with threat intelligence feeds; detection priorities continuously aligned (70-90% aligned)"
      5: "Real-time threat landscape monitoring with automated intel feeds driving detection priority updates (90-100% aligned)"

  # --- Product Owner Engagement ---
  - id: T1-Q8
    tier: 1
    criterion: product_owner_engagement
    type: scale
    question: "Rate the engagement between your detection engineering team and security product/platform owners (e.g., SIEM vendor, EDR team)."
    question_audit: "Rate the engagement between the assessed team and product/platform owners."
    options:
      1: "No engagement; detection needs are never communicated to product/platform teams"
      2: "Occasional ad hoc engagement, typically reactive to issues or missing features (fewer than 4 interactions per year)"
      3: "Quarterly structured engagements to communicate detection needs, request features, and provide feedback"
      4: "Monthly proactive partnership; detection requirements tracked on product roadmaps (>50% of requests on roadmap)"
      5: "Continuous engagement with joint planning, shared success metrics, and detection needs directly influencing product development"

  # --- Release Testing ---
  - id: T1-Q9
    tier: 1
    criterion: release_testing
    type: scale
    question: "Rate the maturity of your detection rule testing before production deployment."
    question_audit: "Rate the maturity of the assessed team's release testing."
    options:
      1: "No testing; rules are deployed to production without validation (<20% tested)"
      2: "Basic manual testing on some rules before deployment; no standardized process or test environment (20-40% tested)"
      3: "Standardized testing with defined test cases and a staging/test environment; most rules validated before deploy (50-70% tested)"
      4: "Comprehensive testing including unit tests, integration tests, and emulation-based validation; rapid deployment for emerging threats (70-90% automated; 24hr critical deployment capability)"
      5: "Continuous automated testing in full CI/CD pipeline; every rule validated before every deployment (90-100% automated)"

  # ============================================================
  # TIER 2: INTERMEDIATE
  # ============================================================

  # --- False Positive Reduction ---
  - id: T2-Q1
    tier: 2
    criterion: false_positive_reduction
    type: scale
    question: "Rate the maturity of your false positive tuning and reduction program."
    question_audit: "Rate the maturity of the assessed team's FP reduction program."
    options:
      1: "Minimal or no tuning; high false positive rates accepted as normal; no FP metrics tracked"
      2: "Reactive tuning when analysts complain about noisy rules; no systematic tracking of FP rates (10-25% FP reduction from baseline)"
      3: "Regular tuning cycles (at least quarterly); FP rates tracked per rule; tuning is documented (25-50% FP reduction)"
      4: "Comprehensive FP management with automated tuning suggestions, risk-based alert scoring, and continuous monitoring (>50% FP reduction)"
      5: "Automated dynamic tuning with ML; near-zero unnecessary alert noise; continuous optimization (>75% FP reduction from baseline)"

  - id: T2-Q2
    tier: 2
    criterion: false_positive_reduction
    type: scale
    question: "What is your estimated false positive reduction from initial baseline across your tuned rules?"
    question_audit: "What is the assessed team's estimated FP reduction from baseline?"
    options:
      1: "Unknown or not measured"
      2: "Less than 25% reduction"
      3: "25-50% reduction with per-rule FP rate tracking"
      4: "50-75% reduction with automated tuning recommendations"
      5: "Over 75% reduction with ML-assisted continuous tuning"

  # --- Gap Analysis ---
  - id: T2-Q3
    tier: 2
    criterion: gap_analysis
    type: scale
    question: "Rate the maturity of your detection coverage gap analysis."
    question_audit: "Rate the maturity of the assessed team's gap analysis."
    options:
      1: "No gap analysis performed; detection coverage gaps are unknown"
      2: "Some gaps identified informally after incidents expose missing detections (1-3 gaps documented; reactive only)"
      3: "Regular analysis against ATT&CK or similar frameworks at least quarterly; gaps documented, prioritized, and tracked (5+ gaps documented and prioritized)"
      4: "Comprehensive analysis integrated with threat modeling and risk assessment; gaps drive the detection roadmap (continuous tracking; integrated into roadmap)"
      5: "Automated gap analysis using coverage mapping tools; real-time dashboards showing detection coverage (automated continuous analysis)"

  - id: T2-Q4
    tier: 2
    criterion: gap_analysis
    type: checklist
    question: "Do you maintain a documented, prioritized list of detection coverage gaps that is updated at least quarterly?"
    question_audit: "Does the assessed team maintain a documented, prioritized gap list updated quarterly?"
    scoring:
      yes_value: 3

  - id: T2-Q5
    tier: 2
    criterion: gap_analysis
    type: checklist
    question: "Are detection coverage gaps formally communicated to stakeholders (leadership, IR, threat intel) at least quarterly?"
    question_audit: "Are the assessed team's gaps communicated to stakeholders at least quarterly?"
    scoring:
      yes_value: 4

  # --- Internal Testing ---
  - id: T2-Q6
    tier: 2
    criterion: internal_testing
    type: scale
    question: "Rate the maturity of your internal detection testing and validation program."
    question_audit: "Rate the maturity of the assessed team's internal testing program."
    options:
      1: "No internal testing; rules are assumed to work once deployed"
      2: "Occasional manual testing of high-priority rules using basic atomic tests (<40% emulation coverage)"
      3: "Regular testing program with attack emulation (e.g., Atomic Red Team, Caldera) covering major detection categories; results documented (40-70% emulation coverage; at least quarterly)"
      4: "Comprehensive testing with automated attack emulation, purple team exercises, and continuous validation (70-90% automated emulation coverage)"
      5: "Continuous automated testing with full emulation coverage and automated regression testing on every rule change (>90% automated coverage)"

  # ============================================================
  # TIER 3: ADVANCED
  # ============================================================

  # --- False Negative Triage ---
  - id: T3-Q1
    tier: 3
    criterion: false_negative_triage
    type: scale
    question: "Rate your team's ability to identify and remediate false negatives (missed detections)."
    question_audit: "Rate the assessed team's ability to detect and remediate false negatives."
    options:
      1: "No FN identification process; missed detections only discovered during incident response"
      2: "Some FNs identified through post-incident reviews; basic tracking of missed detections (50% of tested samples trigger expected alerts)"
      3: "Systematic FN identification through regular testing and validation; root causes analyzed and documented (70-90% trigger rate; 30-50% FN reduction)"
      4: "Comprehensive FN management with automated detection validation, coverage testing, and rapid remediation (90-100% trigger rate; >50% FN reduction)"
      5: "Continuous automated FN detection using real-time validation against live threat samples (near-zero FN rate; >75% FN reduction)"

  - id: T3-Q2
    tier: 3
    criterion: false_negative_triage
    type: scale
    question: "What is your detection trigger rate when tested samples or emulations are run against your rules?"
    question_audit: "What is the assessed team's detection trigger rate on tested samples?"
    options:
      1: "Unknown or not tested"
      2: "Less than 50% of test samples trigger expected detections"
      3: "50-70% trigger rate with root cause analysis on misses"
      4: "70-90% trigger rate with automated validation and remediation"
      5: "Over 90% trigger rate with continuous regression testing"

  # --- External Validation ---
  - id: T3-Q3
    tier: 3
    criterion: external_validation
    type: scale
    question: "Rate the maturity of your external validation program for detection capabilities."
    question_audit: "Rate the maturity of the assessed team's external validation."
    options:
      1: "No external validation; no red team, pentest, or third-party assessment of detection effectiveness"
      2: "Annual penetration test that includes some detection assessment but is not detection-focused (1 exercise per year)"
      3: "Regular external validation through red team engagements or third-party assessments specifically focused on detection effectiveness (>1 per year; findings drive improvements)"
      4: "Multiple external validation exercises annually including red team, purple team, and breach simulation; systematic feedback integration (multiple per year; >70% of findings remediated within 30 days)"
      5: "Continuous BAS (breach and attack simulation) tools and regular adversary emulation exercises with real-time feedback loops"

  # --- Advanced TTP Coverage ---
  - id: T3-Q4
    tier: 3
    criterion: advanced_ttp_coverage
    type: scale
    question: "Rate your detection coverage of advanced TTPs beyond basic signatures and IOCs."
    question_audit: "Rate the assessed team's coverage of advanced TTPs."
    options:
      1: "No advanced TTP coverage; detections are signature/IOC-based only"
      2: "Limited coverage of 1-2 advanced technique categories (e.g., basic PowerShell abuse detection)"
      3: "Growing coverage of 3-4 categories informed by threat intel; behavioral detections supplement signatures (e.g., LOLBin usage, fileless execution, credential access evasion)"
      4: "Comprehensive coverage of 5+ categories including sophisticated evasion, novel attack chains, and emerging threats (e.g., defense evasion/log tampering, lateral movement via legitimate tools, supply chain vectors)"
      5: "Continuous proactive coverage using AI/ML for anomaly detection and automated response to emerging TTPs; real-time advanced TTP detection"

  - id: T3-Q5
    tier: 3
    criterion: advanced_ttp_coverage
    type: scale
    question: "How many of these advanced TTP categories does your team have behavioral detections for: (1) LOLBins/living-off-the-land, (2) fileless malware, (3) credential dumping evasion, (4) defense evasion/log tampering, (5) lateral movement via legitimate tools?"
    question_audit: "How many of these categories does the assessed team have behavioral detections for: LOLBins, fileless malware, credential evasion, defense evasion, lateral movement?"
    options:
      1: "None of these"
      2: "1 of these categories"
      3: "2-3 of these categories"
      4: "4-5 of these categories"
      5: "All 5 plus additional categories (e.g., supply chain attacks, AI-assisted threats)"

  # ============================================================
  # TIER 4: EXPERT
  # ============================================================

  # --- Threat Hunting ---
  - id: T4-Q1
    tier: 4
    criterion: threat_hunting
    type: scale
    question: "Rate the maturity of your threat hunting program."
    question_audit: "Rate the maturity of the assessed team's threat hunting."
    options:
      1: "No proactive hunting; all detection is passive through deployed rules"
      2: "Occasional ad hoc hunting triggered by intel or incidents; findings not systematically converted to detections (fewer than 2 hunts/month; <30% findings converted to rules)"
      3: "Regular structured hunting at least weekly, driven by documented hypotheses from intel or gap analysis; findings feed detection development (weekly hunts; 50-70% of findings integrated into rules)"
      4: "Comprehensive daily hunting program with advanced analytics (e.g., statistical baselining, graph analysis); systematic integration of all findings (daily hunts; >90% findings integrated)"
      5: "Automated real-time hunting augmented by AI/ML; hunting outputs automatically generate detection rule candidates"

  - id: T4-Q2
    tier: 4
    criterion: threat_hunting
    type: checklist
    question: "Are your threat hunts driven by documented hypotheses (from intel, incidents, or gap analysis) rather than ad hoc exploration?"
    question_audit: "Are the assessed team's hunts driven by documented hypotheses?"
    scoring:
      yes_value: 4

  - id: T4-Q3
    tier: 4
    criterion: threat_hunting
    type: checklist
    question: "Is there a defined process to convert threat hunting findings into production detection rules?"
    question_audit: "Is there a defined process to convert hunting findings into production rules?"
    scoring:
      yes_value: 4

  # --- Automation & Continuous Improvement ---
  - id: T4-Q4
    tier: 4
    criterion: automation_continuous_improvement
    type: scale
    question: "What percentage of your detection engineering tasks are automated?"
    question_audit: "What percentage of the assessed team's DE tasks are automated?"
    options:
      1: "None; all processes are manual (0% automated)"
      2: "Basic automation of some repetitive tasks like deployment scripts (<30% automated)"
      3: "Significant lifecycle automation including AI-based quality checks on new rules; improvement metrics tracked (40-60% automated)"
      4: "Advanced automation covering most of the lifecycle; AI/LLM tools used for rule optimization, duplication detection, and analysis (70-80% automated)"
      5: "Full AI/LLM integration throughout the detection lifecycle; automated rule generation, tuning, and retirement (>90% automated; 40%+ FP reduction via AI)"

  - id: T4-Q5
    tier: 4
    criterion: automation_continuous_improvement
    type: scale
    question: "How many detection lifecycle stages have AI/LLM or automation integration? Stages: (1) rule authoring, (2) testing, (3) tuning, (4) deployment, (5) monitoring, (6) retirement."
    question_audit: "How many detection lifecycle stages have automation or AI integration?"
    options:
      1: "None; all stages are manual"
      2: "1 stage (e.g., deployment scripts only)"
      3: "2-3 stages automated or AI-assisted"
      4: "4-5 stages automated or AI-assisted"
      5: "All 6 stages with AI/LLM integration throughout"

  # ============================================================
  # ENRICHMENT: PEOPLE & ORGANIZATION
  # ============================================================

  # --- Team Structure ---
  - id: EP-Q1
    tier: enrichment_people
    criterion: team_structure
    type: scale
    question: "Rate the maturity of your detection engineering team structure."
    question_audit: "Rate the maturity of the assessed detection engineering team structure."
    options:
      1: "No dedicated roles; detection work is a side task for SOC analysts or other staff (0 dedicated FTEs)"
      2: "One or more staff have detection engineering as a partial responsibility; no formal team or career path"
      3: "At least one dedicated detection engineering role or team established with clear responsibilities and defined career progression"
      4: "Established multi-person team with subject matter experts across key domains (host, network, cloud, application); defined career ladder"
      5: "Mature team with deep specialization, mentorship programs, and influence on organizational security strategy"

  # --- Skills Development ---
  - id: EP-Q2
    tier: enrichment_people
    criterion: skills_development
    type: scale
    question: "Rate the maturity of your detection engineering skills development program."
    question_audit: "Rate the maturity of the assessed team's skills development."
    options:
      1: "No formal training; learning is entirely self-directed with no organizational support or budget"
      2: "Ad hoc training; individuals may attend a conference or take a course occasionally but there is no structured program"
      3: "Written training plan with scheduled activities; regular knowledge sharing sessions (e.g., weekly/biweekly); defined skill requirements for roles"
      4: "Comprehensive program covering advanced topics; cross-training with IR, threat intel, and engineering teams; certifications supported and funded"
      5: "Continuous learning culture with community contribution (blog posts, conference talks), internal research programs, and mentorship"

  # --- Leadership Commitment ---
  - id: EP-Q3
    tier: enrichment_people
    criterion: leadership_commitment
    type: scale
    question: "Rate the level of executive sponsorship and leadership commitment to detection engineering."
    question_audit: "Rate the executive sponsorship of the assessed detection engineering function."
    options:
      1: "No executive awareness; detection engineering is not recognized as a distinct capability"
      2: "Some leadership awareness but no formal executive sponsor, no dedicated budget allocation"
      3: "Executive sponsor identified; dedicated budget for tooling and headcount; detection engineering formally recognized as a function"
      4: "Strong executive support; detection engineering metrics included in regular executive reporting; function influences security investment decisions"
      5: "Detection engineering is a strategic priority; board-level visibility; leadership actively champions the function across the organization"

  - id: EP-Q4
    tier: enrichment_people
    criterion: leadership_commitment
    type: checklist
    question: "Does your detection engineering team present metrics or results to executive leadership at least quarterly?"
    question_audit: "Does the assessed team present metrics to executive leadership at least quarterly?"
    scoring:
      yes_value: 3

  - id: EP-Q5
    tier: enrichment_people
    criterion: leadership_commitment
    type: checklist
    question: "Has executive leadership made investment or staffing decisions based on detection engineering metrics or recommendations in the past year?"
    question_audit: "Has leadership made investment/staffing decisions based on the assessed team's metrics in the past year?"
    scoring:
      yes_value: 4

  # ============================================================
  # ENRICHMENT: PROCESS & GOVERNANCE
  # ============================================================

  # --- Detection Lifecycle ---
  - id: EG-Q1
    tier: enrichment_process
    criterion: detection_lifecycle
    type: scale
    question: "Rate the maturity of your detection lifecycle workflow (from request through retirement)."
    question_audit: "Rate the maturity of the assessed team's detection lifecycle workflow."
    options:
      1: "No defined lifecycle; detections created and deployed without structured workflow"
      2: "Basic lifecycle covering creation and deployment only; no formal stages for review, testing, or retirement (2-3 stages defined)"
      3: "Full lifecycle defined and followed: request, development, review, testing, deployment, monitoring, and retirement (all 7 stages documented)"
      4: "Lifecycle enforced through tooling and automation; SLAs defined for each stage; cycle time and throughput metrics tracked"
      5: "Optimized lifecycle with automated stage transitions, predictive analytics for rule retirement, and continuous process improvement"

  # --- Metrics & KPIs ---
  - id: EG-Q2
    tier: enrichment_process
    criterion: metrics_tracking
    type: scale
    question: "Rate the maturity of your detection engineering metrics program."
    question_audit: "Rate the maturity of the assessed team's metrics program."
    options:
      1: "No metrics tracked; success is anecdotal or unmeasured"
      2: "1-2 basic metrics tracked informally (e.g., rule count, alert volume); no formal KPI program or dashboards"
      3: "3-5 defined KPIs covering key areas with quarterly reporting and dashboards (e.g., coverage %, FP rate, deployment velocity)"
      4: "5+ KPIs with automated collection, trending, and correlation; metrics actively drive decision-making and resource allocation"
      5: "Advanced analytics with predictive metrics, industry benchmarking, and data-driven continuous optimization"

  - id: EG-Q3
    tier: enrichment_process
    criterion: metrics_tracking
    type: scale
    question: "How many of these KPI categories does your team actively track: (1) detection coverage (ATT&CK %), (2) detection quality (FP/FN rates), (3) detection velocity (time to deploy new rules), (4) rule health (stale/broken rules), (5) analyst impact (alert-to-incident ratio)?"
    question_audit: "How many of these KPI categories does the assessed team track: coverage, quality, velocity, rule health, analyst impact?"
    options:
      1: "None of these are tracked"
      2: "1 category tracked"
      3: "2-3 categories tracked with regular reporting"
      4: "4-5 categories tracked with automated collection and dashboards"
      5: "All 5 categories plus additional metrics with trend analysis and benchmarking"

  # --- Cross-Team Collaboration ---
  - id: EG-Q4
    tier: enrichment_process
    criterion: cross_team_collaboration
    type: scale
    question: "Rate the maturity of collaboration between detection engineering and other security teams (IR, threat intel, security engineering)."
    question_audit: "Rate cross-team collaboration involving the assessed detection engineering function."
    options:
      1: "Operates in isolation; no structured collaboration with other teams"
      2: "Ad hoc collaboration, typically reactive to incidents or specific requests (no scheduled touchpoints)"
      3: "Regular collaboration through defined channels; scheduled touchpoints at least monthly with IR and threat intel teams"
      4: "Deep integration with joint planning sessions at least quarterly, shared OKRs/objectives, and integrated workflows (e.g., threat intel feeds directly inform detection priorities)"
      5: "Seamless cross-functional collaboration with automated information sharing, shared metrics dashboards, and embedded team members"
